{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sSBfdcEo02EH"
   },
   "source": [
    "## Семинар 8: \"Современные модели для NLP\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yt2LcA_C02EJ"
   },
   "source": [
    "ФИО: Токаева Александра Александровна"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z87HsFGe02EK"
   },
   "source": [
    "### На семинаре мы разберем [код трансфомера на pytorch](https://nlp.seas.harvard.edu/2018/04/03/attention.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F0m8IOq802E8"
   },
   "source": [
    "###  ДЗ [3 балла]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что в этой работе вам потребуется скачать модель весом ~150MB, также ее вычисление занимает определенное время, так что рекомендуется считать эту задачу на [google colab](https://colab.research.google.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "colab_type": "code",
    "id": "6a7Twd_m09PH",
    "outputId": "26dda452-d99a-432b-b9c5-72f370b3c928"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "#!pip install --upgrade transformers\n",
    "from transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5mEU6bzh02E9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/mobilebert-uncased were not used when initializing MobileBertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing MobileBertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MobileBertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "MODEL = (MobileBertForMaskedLM, MobileBertTokenizer, 'google/mobilebert-uncased')\n",
    "\n",
    "model_class, tokenizer_class, pretrained_weights = MODEL\n",
    "# Load pretrained model/tokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "IjX-8e2X1RID",
    "outputId": "9bd4418c-8b25-4551-99e7-86ea334ffc20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2182, 2003, 2070, 3793, 2000, 4372, 16044, 1012, 102]\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\"Here is some text to encode .\", add_special_tokens=True)  # Add special tokens takes care of adding [CLS], [SEP], <s>... tokens in the right way for each model.\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "V72DIYwd1yZS",
    "outputId": "adb668aa-15bb-49f8-fd92-ac1130182d50"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] here is some text to encode [SEP]'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "rXSL-TZG6BF-",
    "outputId": "c0ae498d-d516-4f21-ee22-2719ecc6e176"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] here is some [MASK] to encode [SEP]'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids[4] = tokenizer.mask_token_id\n",
    "tokenizer.decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U1y3f8rh10bz"
   },
   "outputs": [],
   "source": [
    "input_batch = torch.tensor(input_ids).unsqueeze(0) # batch_size 1\n",
    "with torch.no_grad():\n",
    "    res = model(input_batch)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nVwXZBe72Dws"
   },
   "outputs": [],
   "source": [
    "prob = torch.nn.functional.softmax(res, dim=-1)\n",
    "new_ids = prob.max(-1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "6ilhBQmo5r0B",
    "outputId": "914c98e7-8aed-4c4c-de0e-4cccecd58869"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'. here is some way to encode the'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(new_ids.numpy()[0, :].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_TEXTS = [\n",
    "    \"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\",\n",
    "    \"A train carriage containing controlled nuclear materials was stolen in Cincinnati today. Its whereabouts are unknown.\"\n",
    "    ]\n",
    "len(GPT_TEXTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ваша задача - сгенерировать продолжение текстов, на которых демонстрировалась работа GPT-2 с помощью загруженной модели (DistillBERT). Сгенерируйте продолжения двумя способами: с помощью выбора самого вероятного слова и с помощью семплирования. Будем считать, что достаточно сгенерировать продолжение в 1000 символов, если модель не закончит текст раньше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Первый Способ: просто 50 слов подряд"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] in a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the andes mountains. even more surprising to the researchers was the fact that the unicorns spoke perfect english. [SEP] they were also very intelligent and intelligent animals. [SEP] they were very intelligent animals. [CLS] unicorns were very intelligent animals. [CLS] unicorns were very intelligent animals. [SEP] they were very intelligent animals. [CLS] unicorns were very intelligent animals. [CLS] unicorns were very intelligent animals. [SEP] they\n"
     ]
    }
   ],
   "source": [
    "#Первый способ:Сначала генерируем так: 50 штук, и если сгенерировали точку, \n",
    "#то равновероятно дописываем символ [SEP] или [CLS], \n",
    "# и уже этот текст подаем на вход на следующем шаге\n",
    "\n",
    "input_ids = tokenizer.encode(GPT_TEXTS[0], add_special_tokens=True)\n",
    "point_id = tokenizer.encode(\".\", add_special_tokens=False)[0]\n",
    "sep_id=102\n",
    "cls_id=101\n",
    "for i in range(50):\n",
    "    input_ids.append(tokenizer.mask_token_id)\n",
    "\n",
    "    input_batch = torch.tensor(input_ids).unsqueeze(0) # batch_size 1\n",
    "    with torch.no_grad():\n",
    "        res = model(input_batch)[0]\n",
    "    \n",
    "    prob = torch.nn.functional.softmax(res, dim=-1)\n",
    "    new_id = prob.max(-1)[1][0][-1]\n",
    "    input_ids[-1]=new_id\n",
    "    if (new_id==point_id):\n",
    "        a=np.random.rand()\n",
    "        if (a<0.5):\n",
    "            input_ids.append(cls_id)\n",
    "        else:\n",
    "            input_ids.append(sep_id)\n",
    "print(tokenizer.decode(input_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Первый способ плохо работает"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Второй способ: не 50 слов сразу, а 10 раз надо предсказать 5 масок подряд, но в случайном порядке (порядок выбирается с помощью random permutation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] in a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the andes mountains. even more surprising to the researchers was the fact that the unicorns spoke perfect english. [SEP] however, in fact, the researchers could not understand the natural language of the unicorns and the language of the herd of the unicorns. neither was able to understand the other's language and the language of the herd's language. and they\n"
     ]
    }
   ],
   "source": [
    "N=5\n",
    "input_ids = tokenizer.encode(GPT_TEXTS[0], add_special_tokens=True)\n",
    "\n",
    "for m in range(10):\n",
    "    for i in range(N):\n",
    "        input_ids.append(tokenizer.mask_token_id)\n",
    "\n",
    "    for j in np.random.permutation(N):\n",
    "        input_batch = torch.tensor(input_ids).unsqueeze(0) # batch_size 1\n",
    "        with torch.no_grad():\n",
    "            res = model(input_batch)[0]\n",
    "\n",
    "        prob = torch.nn.functional.softmax(res, dim=-1)\n",
    "        new_id = prob.max(-1)[1][0][j+len(input_ids)-N]\n",
    "        input_ids[j+len(input_ids)-N]=new_id\n",
    "\n",
    "print(tokenizer.decode(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lvCPgNEg6XCl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] a train carriage containing controlled nuclear materials was stolen in cincinnati today. its whereabouts are unknown. [SEP] the remaining part of the train carriage is buried by a railroad car. the carriage is located in the city limits of cincinnati, ohio. the train carriage is located in the city limits of cincinnati, ohio. the train carriage is buried by a railroad car\n"
     ]
    }
   ],
   "source": [
    "N=5\n",
    "input_ids = tokenizer.encode(GPT_TEXTS[1], add_special_tokens=True)\n",
    "\n",
    "for m in range(10):\n",
    "    for i in range(N):\n",
    "        input_ids.append(tokenizer.mask_token_id)\n",
    "\n",
    "    for j in np.random.permutation(N):\n",
    "        input_batch = torch.tensor(input_ids).unsqueeze(0) # batch_size 1\n",
    "        with torch.no_grad():\n",
    "            res = model(input_batch)[0]\n",
    "\n",
    "        prob = torch.nn.functional.softmax(res, dim=-1)\n",
    "        new_id = prob.max(-1)[1][0][j+len(input_ids)-N]\n",
    "        input_ids[j+len(input_ids)-N]=new_id\n",
    "\n",
    "print(tokenizer.decode(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#второй способ работает уже лучше!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Третий способ: оставляем random permutation и добавим top p, top k, сэмплирование вместо максимума"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] in a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the andes mountains. even more surprising to the researchers was the fact that the unicorns spoke perfect english. [SEP] they lived in the region following their father'c expedition, as he became known in argentina and peru. while visiting their families of the various millionaires ’ gold mines and their family ’ descendants ’ wealth, the youngsters were told, they believed that\n"
     ]
    }
   ],
   "source": [
    "N=5\n",
    "input_ids = tokenizer.encode(GPT_TEXTS[0], add_special_tokens=True)\n",
    "\n",
    "for m in range(10):\n",
    "    for i in range(N):\n",
    "        input_ids.append(tokenizer.mask_token_id)\n",
    "\n",
    "    for j in np.random.permutation(N):\n",
    "        input_batch = torch.tensor(input_ids).unsqueeze(0) # batch_size 1\n",
    "        with torch.no_grad():\n",
    "            res = model(input_batch)[0]\n",
    "\n",
    "        prob = torch.nn.functional.softmax(res, dim=-1) #1_len(input_ids)_32000\n",
    "        #print(prob.shape)\n",
    "        all_probs=prob[0,j+len(input_ids)-N,:]\n",
    "        order=np.array(np.argsort(all_probs).data.numpy())[::-1]\n",
    "        #print(order)\n",
    "        candidates=[]\n",
    "        distribution=[]\n",
    "        prob_sum=0\n",
    "        k=20\n",
    "        p=0.7\n",
    "        while (len(candidates)<k and prob_sum<p):\n",
    "            candidates.append(order[len(candidates)]) #id-numbers of candidat token\n",
    "            #print(order[len(candidates)])\n",
    "            prob_sum+=prob[0,j+len(input_ids)-N,order[len(candidates)]]\n",
    "            distribution.append(prob[0,j+len(input_ids)-N,order[len(candidates)]])\n",
    "        \n",
    "        sampler=torch.distributions.Categorical(torch.Tensor(distribution))\n",
    "        new_id=candidates[sampler.sample().data.numpy().item()]\n",
    "        \n",
    "        input_ids[j+len(input_ids)-N]=new_id\n",
    "\n",
    "print(tokenizer.decode(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] a train carriage containing controlled nuclear materials was stolen in cincinnati today. its whereabouts are unknown. [SEP] another train trailer was recovered in the 1990s on the market and included in a tourist section for pedestrians and vehicles in cars and other vehicles across the road. the vehicles are lost, empty and missing, and still in storage, and some are stolen from them\n"
     ]
    }
   ],
   "source": [
    "N=5\n",
    "input_ids = tokenizer.encode(GPT_TEXTS[1], add_special_tokens=True)\n",
    "\n",
    "for m in range(10):\n",
    "    for i in range(N):\n",
    "        input_ids.append(tokenizer.mask_token_id)\n",
    "\n",
    "    for j in np.random.permutation(N):\n",
    "        input_batch = torch.tensor(input_ids).unsqueeze(0) # batch_size 1\n",
    "        with torch.no_grad():\n",
    "            res = model(input_batch)[0]\n",
    "\n",
    "        prob = torch.nn.functional.softmax(res, dim=-1) #1_len(input_ids)_32000\n",
    "        #print(prob.shape)\n",
    "        all_probs=prob[0,j+len(input_ids)-N,:]\n",
    "        order=np.array(np.argsort(all_probs).data.numpy())[::-1]\n",
    "        #print(order)\n",
    "        candidates=[]\n",
    "        distribution=[]\n",
    "        prob_sum=0\n",
    "        k=20\n",
    "        p=0.7\n",
    "        while (len(candidates)<k and prob_sum<p):\n",
    "            candidates.append(order[len(candidates)]) #id-numbers of candidat token\n",
    "            #print(order[len(candidates)])\n",
    "            prob_sum+=prob[0,j+len(input_ids)-N,order[len(candidates)]]\n",
    "            distribution.append(prob[0,j+len(input_ids)-N,order[len(candidates)]])\n",
    "        \n",
    "        sampler=torch.distributions.Categorical(torch.Tensor(distribution))\n",
    "        new_id=candidates[sampler.sample().data.numpy().item()]\n",
    "        \n",
    "        input_ids[j+len(input_ids)-N]=new_id\n",
    "\n",
    "print(tokenizer.decode(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Третий способ уже классный!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PkMIDEs002FJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fNHu0Uhf02FV"
   },
   "source": [
    "#### Feedback (опционально)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bBZdRJeB02FW"
   },
   "source": [
    "Здесь вы можете оставить список опечаток из лекции или семинара:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "raw",
    "id": "TNujGvky02FW"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JNp4g0rW02FX"
   },
   "source": [
    "Здесь вы можете оставить комментарии по лекции или семинару:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "raw",
    "id": "DAA7GGwY02FY"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Practice task 8, Advanced NLP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
